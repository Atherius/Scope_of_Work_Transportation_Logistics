{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54713199-8bd5-4f69-86c8-94fc5b2e7f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "log_file_path = \"abfss://bronze@logisticsandtransport.dfs.core.windows.net/logs/processing_log_bronze.log\"\n",
    "logger = logging.getLogger(\"RawDataPipeline\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class ADLSHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            from pyspark.dbutils import DBUtils\n",
    "            dbutils = DBUtils(spark)\n",
    "            message = self.format(record)\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            full_message = f\"[{timestamp}] {message}\\n\"\n",
    "\n",
    "            try:\n",
    "                existing_logs = dbutils.fs.head(log_file_path, 1048576)\n",
    "            except Exception:\n",
    "                existing_logs = \"\"\n",
    "\n",
    "            updated_logs = existing_logs + full_message\n",
    "            dbutils.fs.put(log_file_path, updated_logs, overwrite=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to write log: {e}\")\n",
    "\n",
    "log_handler = ADLSHandler()\n",
    "formatter = logging.Formatter('%(levelname)s - %(message)s')\n",
    "log_handler.setFormatter(formatter)\n",
    "\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(log_handler)\n",
    "\n",
    "logger.info(\"Logging initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7166440-2de1-4d47-87d3-58e8b1d0451d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #importing necessary packages\n",
    "    from pyspark.sql import*\n",
    "    from pyspark.sql.functions import*\n",
    "    import logging\n",
    "    import os\n",
    "    logger.info(\"Library imported successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f571634-1a01-44d5-81b8-29a275c525d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 62 bytes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.conf.set(\n",
    "        \"fs.azure.account.key.logisticsandtransport.dfs.core.windows.net\",\n",
    "        \"AztFZBkLKu6aGzdDi4r7aT7UA7G4UF5oSkFFTwzGjWNM79CRvggV3PuK5iAyZGCbYobManm6J5Ny+AStqinx7A==\")\n",
    "    \n",
    "    spark.conf.set(\n",
    "        \"fs.azure.account.key.transportandlogicticsraw.dfs.core.windows.net\",\n",
    "        \"CzVjwv4Xchg7UsSNkdia5VoTOFHhtvjTsgoZs4ObDP6r5PRczlLMOEcmRE8W+Jafaqhm8ZtDa471+AStlaT3uA==\")\n",
    "    logger.info(\"Acess key verified successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c09cb3ab-1200-4154-bc03-606716bad50a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 115 bytes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #setting up the paths of both raw and bronze data containers from blob and adls respectively\n",
    "    raw_data_path=\"abfss://raw@transportandlogicticsraw.dfs.core.windows.net/\"\n",
    "    bronze_path=\"abfss://bronze@logisticsandtransport.dfs.core.windows.net/\"\n",
    "    logger.info(\"paths set successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9524f8aa-0201-4df8-ad21-f7549b3f38f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 171 bytes.\n",
      "Wrote 232 bytes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    try:\n",
    "        #changing delivery data from csv to parquet\n",
    "        df_delivery=spark.read.format('csv').options(header='true', inferSchema='true').load(f\"{raw_data_path}delivery_data.csv\")\n",
    "        delivery_bronze=df_delivery.withColumn('date_of_ingestion',current_timestamp())\\\n",
    "            .withColumn('source_file',lit('delivery_data.csv'))\n",
    "        delivery_bronze.write.mode('overwrite').parquet(f\"{bronze_path}delivery_data_bronze.parquet\")\n",
    "        logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cba8949-6ac9-4127-a46c-c599adcd54ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 288 bytes.\n",
      "Wrote 349 bytes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    try:\n",
    "        #changing driver data from csv to parquet\n",
    "        df_driver=spark.read.format('csv').options(header='true', inferSchema='true').load(f\"{raw_data_path}driver_data.csv\")\n",
    "        driver_bronze=df_driver.withColumn('date_of_ingestion',current_timestamp())\\\n",
    "            .withColumn('source_file',lit('driver_data.csv'))\n",
    "        driver_bronze.write.mode('overwrite').parquet(f\"{bronze_path}driver_data_bronze.parquet\")\n",
    "        logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f419cd86-f820-4b27-8745-5342ecbd2645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 405 bytes.\n",
      "Wrote 466 bytes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    try:\n",
    "        #changing route data from csv to parquet\n",
    "        df_route=spark.read.format('csv').options(header='true', inferSchema='true').load(f\"{raw_data_path}route_data.csv\")\n",
    "        route_bronze=df_route.withColumn('date_of_ingestion',current_timestamp())\\\n",
    "            .withColumn('source_file',lit('route_data.csv'))\n",
    "        route_bronze.write.mode('overwrite').parquet(f\"{bronze_path}route_data_bronze.parquet\")\n",
    "        logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8f5a91-f9f2-4265-97cc-574248711c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 522 bytes.\n",
      "Wrote 583 bytes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    try:\n",
    "        #changing vehicle data from csv to parquet\n",
    "        df_vehicle=spark.read.format('csv').options(header='true', inferSchema='true').load(f\"{raw_data_path}vehicle_data.csv\")\n",
    "        vehicle_bronze=df_vehicle.withColumn('date_of_ingestion',current_timestamp())\\\n",
    "            .withColumn('source_file',lit('vehicle_data.csv'))\n",
    "        vehicle_bronze.write.mode('overwrite').parquet(f\"{bronze_path}vehicle_data_bronze.parquet\")\n",
    "        logger.info(\"Read operation successful.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in reading data: {e}\")\n",
    "    logger.info(\"Parquet generated successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rawdatahandling_new",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
